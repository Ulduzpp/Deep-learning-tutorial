{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prelude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teaching computers to recognize patterns in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A macine learning technique that learns features and tasks directly from data by running inputs through neural network.\n",
    "\n",
    "* A sub-field of machine learning where algorithms are inspired by the structure of the human brain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning have existed sincr many many years ago, so why does it get so much attention and improvement now? Let's demostrate the reasons.\n",
    "\n",
    "1. Data is prevelent.\n",
    "\n",
    "2. Improved hardware architecture.\n",
    "\n",
    "3. New software architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deep learning models refer to the training of things called neural networks.(from the basis of deep learning)\n",
    "\n",
    "* Just like neurons make up the brain, the fundamental building blocks of a neural network is also a neuron. They take data, Train themselves to understand patterns in the data, and predict outputs for a new set of similar data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Learning Process of Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the last step before propagation a neural network spits out a prediction. This prediction could have two possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How important is that neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allows for the shifting of sigma to the right or left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is propagation of information from the input layer to the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is exactly like forward propagation except in the reverse direction. Backward propagation is why neural networks are so powerful. It is the reason why neural networks can learn by themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which activation function to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sigmoid function works well for binary classification problems, because approximating our classifier functions as combinations of the sigmoid is easier than maybe the ReLU.\n",
    "This will lead to faster training processes and larger convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Non-Linear Activation Functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions serve to intrioduce something called non-linearity in the network. If we use linear activation functions to model a data no matter how many hidden layers we use, it will always be equivalent to having or using a single hidden layer, and in deep learning we want to be able to model any type of data without being restricted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a way that we can quantify the deviation of the predicted output by the neural network to the expected output, in other words it measures how wrong predictions made by neural networks are.\n",
    "We have different kinds of Loss Functions!\n",
    "* Regression: Squared Error, Huber Loss\n",
    "* Binary Classification: Binary Cross-Entropy, Hinge Loss\n",
    "* Multi-Class Classification: Multi-Class Cross-Entropy, Kullback Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers(Minimizing the loss function, trial and error will get you there)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we adjust the parameters to minimize the loss function and make our model as optimized as possible. Optimizers tie together the loss function and model parameters by updating the network based on the output of the loss function. It shapes and molds your model to a more accurate model by adjusting the weights and biases.\n",
    "\n",
    "Loss Functions guide optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent-The Granddaddy of Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An iterative algorithm that starts off at a random point on the loss function and travels down its slope in steps until it reaches the lowest point(minimum) of the function.\n",
    "* Most Popular Optimizer\n",
    "* Fast/Robust/Flexible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It contains three steps:\n",
    " \n",
    "* Calculate what a small change in each individual weight would do to the loss function.\n",
    "* Adjust each parameter based on its gradient(i.e take a small step in the determined direction)\n",
    "* Repeat steps 1 and 2 until the loss function is as low as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gredient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gredient of a function is the vector of partial derivatives with respect to all independent variables. It always points in the direction of the steepest increase in the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In gradient descent, we ideally want to find the global minimum of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid getting stuck in a local minima, we use the proper Learning Rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Usually a small number, like 0.001, that are multiplied to scale the gradients.\n",
    "\n",
    "* Ensures that any changes made to the weights are quite small.\n",
    "\n",
    "* We don't want a large learning rate, where the algorithm will overshoot the global minimum.\n",
    "\n",
    "* Similarly, we don't want one too small, where the algorithm will take forever to converge to the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Like Gradient descent, except uses a subset of training examples rather than the entire lot.\n",
    "* SGD is an implementation of Gradient Descent that uses batches on each pass.\n",
    "* Uses momentum to accumulate gradients.\n",
    "(momentum: accumulates gradients of the past steps to dictate what might happen in the next steps.)\n",
    "* Less expensive computationally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation = Gradient Descent implemented on a network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are many different optimizers based on gradient descent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adapts learning rate to individual features.\n",
    "* Some weights will have different learning rates.\n",
    "* Ideal for sparse datasets with many input examples missing.\n",
    "* Learning rate tends to get small with time.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Is a Specialized version of Adagrad developed by Jeffry Hinton.\n",
    "* Accumulates Gradient in a fixed window.\n",
    "* Similar to Adaprop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adam "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Standa for Adaptive Moment Estimation.\n",
    "* Uses momentum.( our way of telling the neural network whether we want past changes to affect the new change by adding fractions of the previous gradients to the current one.)\n",
    "* Widely spread!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters and Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Model Parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables internal to the neural network. Value can be estimated right from the data.\n",
    "* Required by the model to make oredictions.\n",
    "* Values define the skill of the model.\n",
    "* Estimated directly from data.\n",
    "* Not set manually.\n",
    "* Saved as the learned model or part of it.\n",
    "* i.e : weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Model Hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is Configurations external to the neural network. Value can not be estimated right from data.\n",
    "* No clear-cut way to find the best value.\n",
    "* When a DL algorithm is tuned, you are really tuning the hyperparameters.\n",
    "* If you have to manually specify a parameter, it is a hyperparameter.\n",
    "* i.e: Learning rate, C and sigma in SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoches, Batches, Batch sizes and Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need this terminologies when the data is too big, and we can't pass all the data to the computer at once. So we divide the data to the small chunks and give it to the computer one by one and update the weights of the neural net work at the end of the every step to fit it into the given data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One epoch is when the entire dataset is passed forward and backward through the neural network only once.\n",
    "\n",
    "* We pass the dataset multiple times through the neural network, so it's able to generalize better.\n",
    "\n",
    "* As the number of epoches increases the more the parameters are adjusted leading to a better performing model, but too many epoches can lead to something called overfitting, where a model has essentially memorized the patterns in the training data and performs terribly on the data it's never seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch and Batch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide large datasets into smalller batches and feed those batches into the neural network.\n",
    "\n",
    "* Batch Size : Total number of training examples in a batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of batches needed to complete one epoch.\n",
    "\n",
    "* Number of batches = Number of iterations for one epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning(Predict the correct label for unseen data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Algorithms designed to learn by example, it is like we have a human supervising the whole process.\n",
    "* Models are trained on well-labelled data.\n",
    "* It has two types: Classification(Linear Classifiers, Support Vector Machines, K-Nearest Neighbors, Random Forest), Regression(Linear Regression, Lasso Regression, Multivariate Regression)\n",
    "\n",
    "Each example is a pair consisting of:\n",
    "* Input Object(typically a vector)\n",
    "* Desired Output(Supervisory signal)\n",
    "\n",
    "During training,\n",
    "\n",
    "SL algorithm searches for patterns that correlate with the desired output.\n",
    "\n",
    "After training,\n",
    "\n",
    "it will take new unseen inputs and determine which label the new input will be classified as.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applications of supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Bioinformatics\n",
    "\n",
    "2- Object Recognition\n",
    "\n",
    "3- Spam Detection\n",
    "\n",
    "4- Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Uses to manifest underlying patterns in data\n",
    "* Used in exploratory data analysis\n",
    "* does not use labelled data, rather relies on the data features\n",
    "* Goal: Analyze data and find important underlying patterns.\n",
    "* It can be two types: Clustering, Association  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is simplest and among the most common applications of unsupervised learning. It is the process of grouping given data into different clusters or groups. Classes will contain data points that are as similar as possible to eachother and as dissimalar as possible to data points in other clusters.\n",
    "\n",
    "* It can be broken into partitional clustering and hierrarchical clustering\n",
    "* Partitional clustering : It refers to a set of clustering algorithms where each data point only belongs to one cluster.\n",
    "* Hierarchical clustering : It finda clusters by a system of hierarchies, every data point can belong to many clusters, some cluster may contain smaller clusters within it and this hierarcg=hy system can be organized as a tree diagram.\n",
    "* i.e : K-means, Expectation Maximization, Hierarchical Cluster Analysis(HCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Association"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempts to find relationships between different entities. i.e: market Basket Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applications of unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- AirBnb\n",
    "2- Amazon\n",
    "3- Credit card fraud detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Enables an agent to learn in an interactive environment by trial & error based on feedback from its own actions & experiences.\n",
    "* Like supervised learning it uses mapping between the input and output but unlike it where the feedback provided to the agent is a correct set of actions for performing a task, Reinforcement learning uses rewards and punishments as signals for positive and negative behaviour.\n",
    "* • Goal: Find a suitable model that would maximize the\n",
    "total cumulative reward.\n",
    "* It refers to goal oriented algorithms.\n",
    "* Usually modelled as a Markov Decision Process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applications of Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Robotics\n",
    "\n",
    "2- Business Strategy Planning\n",
    "\n",
    "3- Traffic Light Control\n",
    "\n",
    "4- Web System Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core Problem in Deep Learning : \n",
    "\n",
    "* Model should perform well on training data AND new test data.\n",
    "\n",
    "* Most common problem faced is Overfitting!\n",
    "\n",
    "When the algorithm doesn't fit the data well, it is called underfitting, it underestimates data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tackling Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Drop out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What drop out does is that at every iteration it randomly selects some nodes and removes them with their incoming and outgoing connections as shown, so each iteration has a different set of nodes and this results in a different set of outputs. \n",
    "\n",
    "So why do these models perform better?\n",
    "\n",
    "These models usually perform better than a single model as they capture more randomness and memorizes less of the training data and hence will be folks to generalize better and build a more robust predictive model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Dataset Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying transformations on the existing dataset to get synthesize more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training error decreases steadily, But, validation error increases after a certain point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each neuron is connected to every subsequent layer with no backward connections. There are no cycles or loops in connections in the network. It takes in fixed-sized inputs and returns fixed-sized outputs.\n",
    "* More neurons -> Larger Computational Resources\n",
    "* vanilla feed forward neural networks cannot model sequential model.(sequential data is a data in a sequence.)\n",
    "* vanilla NNs don't share parameters across time.\n",
    "* Sharing parameters gives the network the ability to look for a given feature everywhere in the sequence, rather than in just a cetain area.\n",
    "\n",
    "So we need a network to:\n",
    " * Deal with variable length sequences.\n",
    " * Maintain sequence order.\n",
    " * Keep tracxk of long-term dependencies.\n",
    " * Share parameters across the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Uses a feedback loop in the hidden layers.\n",
    "* Unlike NN, RNN can operate effectively on sequences of data with variable input length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Uses the Backpropagation algorithm.\n",
    "* Backprop applied for every sequence data point.\n",
    "* Backpropagation through Time(BTT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Vanishing Gradient Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short term memory is caused by the infamous vanishing and exploding graient problems, as the rnn processes more words, it has trouble retaining information from previous steps, kinda like our memory. Short term memory and vaqnishing gradient is due to the nature of bak propagation, The algorithm used to train and optimize neural networks.\n",
    "\n",
    "* Gradients of a layer are calculated based on the gradients of the previous layer.\n",
    "* If initial gradient is small, adjustments to the subsequent layers will be smaller giving rise to vanishing gradients.\n",
    "* Gradients used make adjustments to weights and biases. Because of VGP, RNNs are unable to learn long-range dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applications of RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN work well for applications that involve sequences of data that change over time. \n",
    "\n",
    "* Natural Language Processing\n",
    "\n",
    "* Sentiment Analysis\n",
    "\n",
    "* DNA Sequence Classification\n",
    "\n",
    "* Speech Recognition\n",
    "\n",
    "* Language Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMs & GRNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM - Long Short Term Memory\n",
    "GRNN - Gated RNN\n",
    "* Capable of learning long-term dependencies using gates.\n",
    "\n",
    "* These gates are different tensor operations that learn information that can learn what information to add or remove to the hidden state or the feedback loop.\n",
    "\n",
    "* The main difference between a gated RNN and an LSTM is that the gated RNN has two gates to control its memory(1. Update Gate 2. Reset Gate) while LSTM has three gates(1. Update Gate 2. Reset Gate 3. Forget Gate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is a type of neural network architecture designed for specific tasks like image classification.\n",
    "\n",
    "* They are inspired by the organization of neurons in the visual cortex of the human brain, and it is good for processing data like images, audio and video.\n",
    "\n",
    "* CNNs derive their names from the type of hidden layers it consists of. the hidden layers of a CNN typically consist of convolutional layers, poling layers, fully-connected layers and normalization layers. This means that instead of traditional activation functions we use in feed forward neural networks, convolution and pooling functions instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convolution is a technique that allows us to extract visual features from a 2d array. Each neuron in a convolutional layer is responsible for a small cluster of neurons in the preceding layer. The bounding box that determines the cluster of neurons is called a filter(kernel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling also known as sub sampling and down sampling is the next step in a convolutional neural network.\n",
    "\n",
    "* Its objective is to further reduce the number of neurons necessary in subsequent layers of the network, while still retaining the most important information. \n",
    "\n",
    "* It has two types: 1. Max Pooling  2. Min Pooling\n",
    "\n",
    "* Step in object classification with CNN : 1. Convolve the image    2. Pool the result   3. Repeat  4. then we add a few fully connected hidden layers to help classify the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applications of CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Computer Vision\n",
    "\n",
    "* Image Recognition\n",
    "\n",
    "* Image Processing\n",
    "\n",
    "* Image Segmentation\n",
    "\n",
    "* Video Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to discuss the 5 steps that are common in every deep learning project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data is at the core of what deep learning is all about. Your model will only be as powerful as the data you bring.\n",
    " \n",
    "* Picking the right data is key.\n",
    "\n",
    "* Bad data = bad model\n",
    "\n",
    "* Make assumptions about the data you need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amounts of data you need = 10X  Number of model parameters\n",
    "\n",
    "However this may differ from time to time depending on the type of model you are building.\n",
    "\n",
    "* Regression : 10 examples per predictor variable.\n",
    "\n",
    "* Image Classification : 1000 images per class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reliability\n",
    "\n",
    "* How common are labelling errors?\n",
    "\n",
    "* Are your features noisy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has steps: \n",
    "\n",
    "1. Splitting dataset into subsets\n",
    "\n",
    "* Train on training data\n",
    "\n",
    "* Evaluate on validation data:\n",
    "\n",
    " Developing a model involves tuning its configuration in other words choosing\n",
    "\n",
    " certain values for the hyper parameters or the weight and biases, this tuning is\n",
    "\n",
    " done with the feedback recieved from the vaslidation set and is in essence a form\n",
    "\n",
    " of learning, it turns out we can't just split the dataset randomly! Do that and\n",
    "\n",
    " you will get random results, there has to be some kind of logic to split the\n",
    "\n",
    " dataset. Specifying the size of validation set dependa on two factors: 1. Number\n",
    " \n",
    " of samples in the data.  2. Model being trained\n",
    "\n",
    " Few hyperparameters -> Small validation set \n",
    "\n",
    " Many hyperparameters -> Large validation set\n",
    "\n",
    " No hyperparameters or ines that cannot be easily tuned -> No validation set \n",
    "\n",
    " required!\n",
    "\n",
    " train-test-validation split ratio is specific to your use-case.\n",
    "\n",
    " Time splits are best suited for large datasets.\n",
    " \n",
    "* Test it on testing data\n",
    "\n",
    "\n",
    "2. Formatting\n",
    "\n",
    " The dataset you have picked might not be in the format that you like, for \n",
    "\n",
    " example the data might be in the form of a database, but you'd like it as a csv\n",
    "\n",
    " file.\n",
    "\n",
    "3. Missing Data\n",
    "\n",
    " 1. Eliminating features with missing values\n",
    "\n",
    " 2. Imputing the missing value\n",
    "\n",
    "4. Sampling \n",
    "\n",
    " Use a small sample of the dataset.\n",
    "\n",
    " * Especially when we have a classification model where in the data certain class\n",
    "\n",
    " is a lot more than the other one(s). We use down sampling.\n",
    "\n",
    " Example weight = original weight x Downsampling factor\n",
    "\n",
    " The benefits of downsampling?\n",
    "\n",
    " 1. Faster convergence\n",
    "\n",
    " 2. Reduced disk space\n",
    "\n",
    " 3. dataset is in similar ratio\n",
    "\n",
    "5. Feature Scaling \n",
    "\n",
    " It is a very crucial step in deep learning as the deep learning algorithms perform \n",
    " \n",
    " much better when dealing with features that are on the same scale.\n",
    "\n",
    " 1. Normalization : Rescaling of features to arange between 0 and 1, which in fact \n",
    "  \n",
    " is a special case of min max scaling. To normalize that data we need to apply min\n",
    "\n",
    " max scaling to each feature column.\n",
    "\n",
    " 2. Standardization : Consists of centering the field at mean 0 with standard\n",
    "\n",
    " deviation 1 so that feature columns have the same parameters as a standard normal\n",
    "\n",
    " distribution. This makes it easier for the learning algorithms to learn weights of\n",
    " \n",
    " the parameters, in addition it keeps useful information about outliers and make \n",
    " \n",
    " algorithms less sensitive to them.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Feed Data\n",
    "\n",
    "* Forward Propagation \n",
    "\n",
    "* Loss Function \n",
    "\n",
    "* Back Propagation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model on the validation set meant to be representative of the model in the real\n",
    "\n",
    "world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Optimizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By increasing the number of epoches, in other words\n",
    "\n",
    "showing the entire dataset multiple times. This somtimes have shown to improve \n",
    "\n",
    "accuracy, in other ways by adjusting the learning rate(It defines how far we shift\n",
    "\n",
    "the line during each step based on information from the previous training step in\n",
    "\n",
    "backpropagation.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Addressing Overfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to avoid overfitting, getting more\n",
    "\n",
    "data and regularization. \n",
    "\n",
    "* Getting more data is usually the best solution, a model trained on more data, will\n",
    "\n",
    "naturally generalize better. \n",
    "\n",
    "* Reducing the model size by reducing the number of learnable parameters in the model\n",
    "\n",
    "and with it, its learning capacity is another way, however by lowering the capacity\n",
    "\n",
    "of the network, you force it to learn the patterns that matter. On other hand\n",
    "\n",
    "lowering the networks capacity too much will lead to underfitting, the model will\n",
    "\n",
    "not be able to learn the relevant patterns in the trained data.\n",
    "\n",
    "* Applying wieght regularization to model : a common way to achieve this is to\n",
    "\n",
    "constraint complexity of the network, by forcing its weights to take only small \n",
    "\n",
    "values, regularizing the distribution of weight values. This is done by adding to \n",
    "\n",
    "the loss function of the network, a cost associated with having larger weights and\n",
    "\n",
    "this cost comes in two ways:\n",
    "\n",
    "1. L1 Regularization at the cost with regards to the absolute value of the weight\n",
    " \n",
    "coefficiant, or the L1 norm of the weights.\n",
    "\n",
    "2. L2 Regularization adds a cost with regards to the squared value of the weight's\n",
    "\n",
    "coefficient that is the L2 norm of the weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flipping, Blurring, Zooming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Drop out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a technique used in deep learning that randomly drops out units or neurons in\n",
    "\n",
    "the network.\n",
    "\n",
    "* So why do we need drop out at all?\n",
    "\n",
    "A fully connected layer occupies most of the parameters and hence neurons develop a \n",
    "\n",
    "co-dependency amongst each other during training, which curbs the individual power \n",
    "\n",
    "of each neuron and which altimately leads to overfitting of the training data so \n",
    "\n",
    "drop out is a good way of reducing overfitting."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
